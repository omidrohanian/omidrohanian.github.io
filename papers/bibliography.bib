@article{ROHANIAN2024103007,
title = {Exploring the effectiveness of instruction tuning in biomedical language processing},
journal = {Artificial Intelligence in Medicine},
volume = {158},
pages = {103007},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103007},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002495},
author = {Omid Rohanian and Mohammadmahdi Nouriborji and Samaneh Kouchaki and Farhad Nooralahzadeh and Lei Clifton and David A. Clifton},
keywords = {Instruction tuning, Biomedical NLP, Named entity recognition, Relation extraction, Medical NLI, Llama2-MedTuned},
abstract = {Large Language Models (LLMs), particularly those similar to ChatGPT, have significantly influenced the field of Natural Language Processing (NLP). While these models excel in general language tasks, their performance in domain-specific downstream tasks such as biomedical and clinical Named Entity Recognition (NER), Relation Extraction (RE), and Medical Natural Language Inference (NLI) is still evolving. In this context, our study investigates the potential of instruction tuning for biomedical language processing, applying this technique to two general LLMs of substantial scale. We present a comprehensive, instruction-based model trained on a dataset that consists of approximately 200,000 instruction-focused samples. This dataset represents a carefully curated compilation of existing data, meticulously adapted and reformatted to align with the specific requirements of our instruction-based tasks. This initiative represents an important step in utilising such models to achieve results on par with specialised encoder-only models like BioBERT and BioClinicalBERT for various classical biomedical NLP tasks. Our work includes an analysis of the dataset’s composition and its impact on model performance, providing insights into the intricacies of instruction tuning. By sharing our codes, models, and the distinctively assembled instruction-based dataset, we seek to encourage ongoing research and development in this area.22Our code repository is available at https://github.com/nlpie-research/BioInstTune-LLM.}
}

@article{TAYLOR2024103002,
title = {Efficiency at scale: Investigating the performance of diminutive language models in clinical tasks},
journal = {Artificial Intelligence in Medicine},
volume = {157},
pages = {103002},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103002},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002446},
author = {Niall Taylor and Upamanyu Ghose and Omid Rohanian and Mohammadmahdi Nouriborji and Andrey Kormilitzin and David A. Clifton and Alejo Nevado-Holgado},
keywords = {Large language models, Artificial intelligence, PEFT, Fine-tuning, NLP},
abstract = {The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability. This was followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as 25 million parameters. Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can operate on low-cost, in-house computing infrastructure. The advantages of these models, in terms of speed and reduced training costs, dramatically outweighs any performance gain from large foundation LLMs. Furthermore, we highlight how domain-specific pre-training interacts with PEFT methods and model size, finding the domain pre-training to be particularly important in smaller models and discuss how these factors interplay to provide the best efficiency-performance trade-off. Full code available at: https://github.com/nlpie-research/efficient-ml.}
}

@article{seminog2024protocol,
  title={A protocol for a living mapping review of global research funding for infectious diseases with a pandemic potential--Pandemic PACT},
  author={Seminog, Olena and Furst, Rodrigo and Mendy, Thomas and Rohanian, Omid and Levanita, Shanthi and Kadri-Alabi, Zaharat and Jabin, Nusrat and Humphreys, Georgina and Antonio, Emilia and Bucher, Adrian and others},
  journal={Wellcome Open Research},
  volume={9},
  pages={156},
  year={2024}
}

@article{rohanian2024lightweight,
  title={Lightweight transformers for clinical natural language processing},
  author={Rohanian, Omid and Nouriborji, Mohammadmahdi and Jauncey, Hannah and Kouchaki, Samaneh and Nooralahzadeh, Farhad and Clifton, Lei and Merson, Laura and Clifton, David A and ISARIC Clinical Characterisation Group and others},
  journal={Natural language engineering},
  volume={30},
  number={5},
  pages={887--914},
  year={2024},
  publisher={Cambridge University Press}
}

@article{rohanian2024rapid,
  title={Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine},
  author={Rohanian, Omid and Nouriborji, Mohammadmahdi and Seminog, Olena and Furst, Rodrigo and Mendy, Thomas and Levanita, Shanthi and Kadri-Alabi, Zaharat and Jabin, Nusrat and Toale, Daniela and Humphreys, Georgina and others},
  journal={arXiv preprint arXiv:2407.10086},
  year={2024}
}

@article{chauhan2024continuous,
  title={Continuous patient state attention model for addressing irregularity in electronic health records},
  author={Chauhan, Vinod Kumar and Thakur, Anshul and O’Donoghue, Odhran and Rohanian, Omid and Molaei, Soheila and Clifton, David A},
  journal={BMC Medical Informatics and Decision Making},
  volume={24},
  number={1},
  pages={117},
  year={2024},
  publisher={Springer}
}

@inproceedings{liu-etal-2024-large,
    title = "Large Language Models Are Poor Clinical Decision-Makers: A Comprehensive Benchmark",
    author = "Liu, Fenglin  and
      Li, Zheng  and
      Zhou, Hongjian  and
      Yin, Qingyu  and
      Yang, Jingfeng  and
      Tang, Xianfeng  and
      Luo, Chen  and
      Zeng, Ming  and
      Jiang, Haoming  and
      Gao, Yifan  and
      Nigam, Priyanka  and
      Nag, Sreyashi  and
      Yin, Bing  and
      Hua, Yining  and
      Zhou, Xuan  and
      Rohanian, Omid  and
      Thakur, Anshul  and
      Clifton, Lei  and
      Clifton, David A.",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.759/",
    doi = "10.18653/v1/2024.emnlp-main.759",
    pages = "13696--13710",
    abstract = "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs"
}

@inproceedings{rohanian-etal-2023-disfluent,
    title = "Disfluent Cues for Enhanced Speech Understanding in Large Language Models",
    author = "Rohanian, Morteza  and
      Nooralahzadeh, Farhad  and
      Rohanian, Omid  and
      Clifton, David  and
      Krauthammer, Michael",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.238/",
    doi = "10.18653/v1/2023.findings-emnlp.238",
    pages = "3676--3684",
    abstract = "In computational linguistics, the common practice is to ``clean'' disfluent content from spontaneous speech. However, we hypothesize that these disfluencies might serve as more than mere noise, potentially acting as informative cues. We use a range of pre-trained models for a reading comprehension task involving disfluent queries, specifically featuring different types of speech repairs. The findings indicate that certain disfluencies can indeed improve model performance, particularly those stemming from context-based adjustments. However, large-scale language models struggle to handle repairs involving decision-making or the correction of lexical or syntactic errors, suggesting a crucial area for potential improvement. This paper thus highlights the importance of a nuanced approach to disfluencies, advocating for their potential utility in enhancing model performance rather than their removal."
}

@article{10.1093/bioinformatics/btad103,
    author = {Rohanian, Omid and Nouriborji, Mohammadmahdi and Kouchaki, Samaneh and Clifton, David A},
    title = {On the effectiveness of compact biomedical transformers},
    journal = {Bioinformatics},
    volume = {39},
    number = {3},
    pages = {btad103},
    year = {2023},
    month = {02},
    abstract = {Language models pre-trained on biomedical corpora, such as BioBERT, have recently shown promising results on downstream biomedical tasks. Many existing pre-trained models, on the other hand, are resource-intensive and computationally heavy owing to factors such as embedding size, hidden dimension and number of layers. The natural language processing community has developed numerous strategies to compress these models utilizing techniques such as pruning, quantization and knowledge distillation, resulting in models that are considerably faster, smaller and subsequently easier to use in practice. By the same token, in this article, we introduce six lightweight models, namely, BioDistilBERT, BioTinyBERT, BioMobileBERT, DistilBioBERT, TinyBioBERT and CompactBioBERT which are obtained either by knowledge distillation from a biomedical teacher or continual learning on the Pubmed dataset. We evaluate all of our models on three biomedical tasks and compare them with BioBERT-v1.1 to create the best efficient lightweight models that perform on par with their larger counterparts.We trained six different models in total, with the largest model having 65 million in parameters and the smallest having 15 million; a far lower range of parameters compared with BioBERT’s 110M. Based on our experiments on three different biomedical tasks, we found that models distilled from a biomedical teacher and models that have been additionally pre-trained on the PubMed dataset can retain up to 98.8\% and 98.6\% of the performance of the BioBERT-v1.1, respectively. Overall, our best model below 30 M parameters is BioMobileBERT, while our best models over 30 M parameters are DistilBioBERT and CompactBioBERT, which can keep up to 98.2\% and 98.8\% of the performance of the BioBERT-v1.1, respectively.Codes are available at: https://github.com/nlpie-research/Compact-Biomedical-Transformers. Trained models can be accessed at: https://huggingface.co/nlpie.},
    issn = {1367-4811},
    doi = {10.1093/bioinformatics/btad103},
    url = {https://doi.org/10.1093/bioinformatics/btad103},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/39/3/btad103/49571998/btad103.pdf},
}

@article{rohanian2022privacy,
  title={Privacy-aware early detection of COVID-19 through adversarial training},
  author={Rohanian, Omid and Kouchaki, Samaneh and Soltan, Andrew and Yang, Jenny and Rohanian, Morteza and Yang, Yang and Clifton, David},
  journal={IEEE journal of biomedical and health informatics},
  volume={27},
  number={3},
  pages={1249--1258},
  year={2022},
  publisher={IEEE}
}

@inproceedings{rohanian-etal-2023-using,
    title = "Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints",
    author = "Rohanian, Omid  and
      Jauncey, Hannah  and
      Nouriborji, Mohammadmahdi  and
      Kumar, Vinod  and
      Gonalves, Bronner P.  and
      Kartsonaki, Christiana  and
      Clinical Characterisation Group, Isaric  and
      Merson, Laura  and
      Clifton, David",
    editor = "Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin",
    booktitle = "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.bionlp-1.5/",
    doi = "10.18653/v1/2023.bionlp-1.5",
    pages = "62--78",
    abstract = "Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not. Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining."
}

@inproceedings{nouriborji-etal-2023-minialbert,
    title = "{M}ini{ALBERT}: Model Distillation via Parameter-Efficient Recursive Transformers",
    author = "Nouriborji, Mohammadmahdi  and
      Rohanian, Omid  and
      Kouchaki, Samaneh  and
      Clifton, David A.",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.83/",
    doi = "10.18653/v1/2023.eacl-main.83",
    pages = "1161--1173",
    abstract = "Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as overparameterisation. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the application of bottleneck adapters for layer-wise adaptation of our recursive student, and also explore the efficacy of adapter tuning for fine-tuning of compact models. We test our proposed models on a number of general and biomedical NLP tasks to demonstrate their viability and compare them with the state-of-the-art and other existing compact models. All the codes used in the experiments and the pre-trained compact models will be made publicly available."
}

@inproceedings{nouriborji-etal-2022-nowruz,
    title = "Nowruz at {S}em{E}val-2022 Task 7: Tackling Cloze Tests with Transformers and Ordinal Regression",
    author = "Nouriborji, Mohammadmahdi  and
      Rohanian, Omid  and
      Clifton, David",
    editor = "Emerson, Guy  and
      Schluter, Natalie  and
      Stanovsky, Gabriel  and
      Kumar, Ritesh  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      Singh, Siddharth  and
      Ratan, Shyam",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.151/",
    doi = "10.18653/v1/2022.semeval-1.151",
    pages = "1071--1077",
    abstract = "This paper outlines the system using which team Nowruz participated in SemEval 2022 Task 7 ``Identifying Plausible Clarifications of Implicit and Underspecified Phrases'' for both subtasks A and B. Using a pre-trained transformer as a backbone, the model targeted the task of multi-task classification and ranking in the context of finding the best fillers for a cloze task related to instructional texts on the website Wikihow. The system employed a combination of two ordinal regression components to tackle this task in a multi-task learning scenario. According to the official leaderboard of the shared task, this system was ranked 5th in the ranking and 7th in the classification subtasks out of 21 participating teams. With additional experiments, the models have since been further optimised. The code used in the experiments is going to be publicly available."
}

@article{soltan2022real,
  title={Real-world evaluation of AI driven COVID-19 triage for emergency admissions: External validation \& operational assessment of lab-free and high-throughput screening solutions},
  author={Soltan, A and Yang, J and Pattanshetty, R and Novak, A and Yang, Y and Rohanian, O and Beer, S and Soltan, M and Thickett, D and Fairhead, R and others},
  journal={Lancet Digital Health},
  volume={4},
  number={4},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{rohanian-etal-2020-verbal,
    title = "Verbal Multiword Expressions for Identification of Metaphor",
    author = "Rohanian, Omid  and
      Rei, Marek  and
      Taslimipoor, Shiva  and
      Ha, Le An",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.259/",
    doi = "10.18653/v1/2020.acl-main.259",
    pages = "2890--2895",
    abstract = "Metaphor is a linguistic device in which a concept is expressed by mentioning another. Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics. Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models. This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs. To the best of our knowledge, this is the first ``MWE-aware'' metaphor identification system paving the way for further experiments on the complex interactions of these phenomena. The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets."
}

@article{rohanian2020contributions,
  title={Contributions to the Computational Treatment of Non-literal Language},
  author={Rohanian, Omid},
  year={2020},
  publisher={University of Wolverhampton},
  url = "https://wlv.openrepository.com/server/api/core/bitstreams/c8f50e97-e3d2-4c7b-9333-1ce9fe8a9a0a/content"
}

@inproceedings{taslimipoor-etal-2019-cross,
    title = "Cross-lingual Transfer Learning and Multitask Learning for Capturing Multiword Expressions",
    author = "Taslimipoor, Shiva  and
      Rohanian, Omid  and
      Ha, Le An",
    editor = "Savary, Agata  and
      Escart{\'i}n, Carla Parra  and
      Bond, Francis  and
      Mitrovi{\'c}, Jelena  and
      Mititelu, Verginica Barbu",
    booktitle = "Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5119/",
    doi = "10.18653/v1/W19-5119",
    pages = "155--161",
    abstract = "Recent developments in deep learning have prompted a surge of interest in the application of multitask and transfer learning to NLP problems. In this study, we explore for the first time, the application of transfer learning (TRL) and multitask learning (MTL) to the identification of Multiword Expressions (MWEs). For MTL, we exploit the shared syntactic information between MWE and dependency parsing models to jointly train a single model on both tasks. We specifically predict two types of labels: MWE and dependency parse. Our neural MTL architecture utilises the supervision of dependency parsing in lower layers and predicts MWE tags in upper layers. In the TRL scenario, we overcome the scarcity of data by learning a model on a larger MWE dataset and transferring the knowledge to a resource-poor setting in another language. In both scenarios, the resulting models achieved higher performance compared to standard neural approaches."
}

@inproceedings{taslimipoor-etal-2019-gcn,
    title = "{GCN}-Sem at {S}em{E}val-2019 Task 1: Semantic Parsing using Graph Convolutional and Recurrent Neural Networks",
    author = "Taslimipoor, Shiva  and
      Rohanian, Omid  and
      Mo{\v{z}}e, Sara",
    editor = "May, Jonathan  and
      Shutova, Ekaterina  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.",
    booktitle = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S19-2014/",
    doi = "10.18653/v1/S19-2014",
    pages = "102--106",
    abstract = "This paper describes the system submitted to the SemEval 2019 shared task 1 `Cross-lingual Semantic Parsing with UCCA'. We rely on the semantic dependency parse trees provided in the shared task which are converted from the original UCCA files and model the task as tagging. The aim is to predict the graph structure of the output along with the types of relations among the nodes. Our proposed neural architecture is composed of Graph Convolution and BiLSTM components. The layers of the system share their weights while predicting dependency links and semantic labels. The system is applied to the CONLLU format of the input data and is best suited for semantic dependency parsing."
}

@inproceedings{rohanian-etal-2019-bridging,
    title = "{B}ridging the Gap: {A}ttending to Discontinuity in Identification of Multiword Expressions",
    author = "Rohanian, Omid  and
      Taslimipoor, Shiva  and
      Kouchaki, Samaneh  and
      Ha, Le An  and
      Mitkov, Ruslan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1275/",
    doi = "10.18653/v1/N19-1275",
    pages = "2692--2698",
    abstract = "We introduce a new method to tag Multiword Expressions (MWEs) using a linguistically interpretable language-independent deep learning architecture. We specifically target discontinuity, an under-explored aspect that poses a significant challenge to computational treatment of MWEs. Two neural architectures are explored: Graph Convolutional Network (GCN) and multi-head self-attention. GCN leverages dependency parse information, and self-attention attends to long-range relations. We finally propose a combined model that integrates complementary information from both, through a gating mechanism. The experiments on a standard multilingual dataset for verbal MWEs show that our model outperforms the baselines not only in the case of discontinuous MWEs but also in overall F-score."
}

@article{taslimipoor2018shoma,
  title={Shoma at parseme shared task on automatic identification of vmwes: Neural multiword expression tagging with high generalisation},
  author={Taslimipoor, Shiva and Rohanian, Omid},
  journal={arXiv preprint arXiv:1809.03056},
  year={2018}
}

@inproceedings{taslimipoor-etal-2018-wolves,
    title = "Wolves at {S}em{E}val-2018 Task 10: Semantic Discrimination based on Knowledge and Association",
    author = "Taslimipoor, Shiva  and
      Rohanian, Omid  and
      Ha, Le An  and
      Corpas Pastor, Gloria  and
      Mitkov, Ruslan",
    editor = "Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      May, Jonathan  and
      Shutova, Ekaterina  and
      Bethard, Steven  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 12th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S18-1160/",
    doi = "10.18653/v1/S18-1160",
    pages = "972--976",
    abstract = "This paper describes the system submitted to SemEval 2018 shared task 10 `Capturing Dicriminative Attributes'. We use a combination of knowledge-based and co-occurrence features to capture the semantic difference between two words in relation to an attribute. We define scores based on association measures, ngram counts, word similarity, and ConceptNet relations. The system is ranked 4th (joint) on the official leaderboard of the task."
}

@inproceedings{taslimipoor2018identification,
  title={Identification of multiword expressions: A fresh look at modelling and evaluation},
  author={Taslimipoor, Shiva and Rohanian, Omid and Mitkov, Ruslan and Fazly, Afsaneh},
  booktitle={Multiword expressions at length and in depth: Extended papers from the MWE 2017 workshop},
  volume={2},
  pages={299},
  year={2018},
  organization={Language Science Press}
}

@inproceedings{taslimipoor-etal-2018-wolves,
    title = "Wolves at {S}em{E}val-2018 Task 10: Semantic Discrimination based on Knowledge and Association",
    author = "Taslimipoor, Shiva  and
      Rohanian, Omid  and
      Ha, Le An  and
      Corpas Pastor, Gloria  and
      Mitkov, Ruslan",
    editor = "Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      May, Jonathan  and
      Shutova, Ekaterina  and
      Bethard, Steven  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 12th International Workshop on Semantic Evaluation",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S18-1160/",
    doi = "10.18653/v1/S18-1160",
    pages = "972--976"
}

@inproceedings{yaneva-etal-2017-combining,
    title = "Combining Multiple Corpora for Readability Assessment for People with Cognitive Disabilities",
    author = "Yaneva, Victoria  and
      Or{\u{a}}san, Constantin  and
      Evans, Richard  and
      Rohanian, Omid",
    editor = "Tetreault, Joel  and
      Burstein, Jill  and
      Leacock, Claudia  and
      Yannakoudakis, Helen",
    booktitle = "Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5013/",
    doi = "10.18653/v1/W17-5013",
    pages = "121--132"
}

@inproceedings{rohanian-etal-2017-using,
    title = "Using Gaze Data to Predict Multiword Expressions",
    author = "Rohanian, Omid  and
      Taslimipoor, Shiva  and
      Yaneva, Victoria  and
      Ha, Le An",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R17-1078/",
    doi = "10.26615/978-954-452-049-6_078",
    pages = "601--609"
}

@inproceedings{taslimipoor-etal-2017-investigating,
    title = "Investigating the Opacity of Verb-Noun Multiword Expression Usages in Context",
    author = "Taslimipoor, Shiva  and
      Rohanian, Omid  and
      Mitkov, Ruslan  and
      Fazly, Afsaneh",
    editor = "Markantonatou, Stella  and
      Ramisch, Carlos  and
      Savary, Agata  and
      Vincze, Veronika",
    booktitle = "Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017)",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1718/",
    doi = "10.18653/v1/W17-1718",
    pages = "133--138"
}

@inproceedings{yaneva2017cognitive,
  title={Cognitive processing of multiword expressions in native and non-native speakers of english: Evidence from gaze data},
  author={Yaneva, Victoria and Taslimipoor, Shiva and Rohanian, Omid and Ha, Le An},
  booktitle={Computational and Corpus-Based Phraseology: Second International Conference, Europhras 2017, London, UK, November 13-14, 2017, Proceedings 2},
  pages={363--379},
  year={2017},
  organization={Springer}
}