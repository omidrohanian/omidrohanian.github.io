{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152e836b-e704-4e4f-a1a0-cc1015def399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:16:26.904408Z",
     "iopub.status.busy": "2025-09-17T14:16:26.903548Z",
     "iopub.status.idle": "2025-09-17T14:16:26.912821Z",
     "shell.execute_reply": "2025-09-17T14:16:26.911670Z",
     "shell.execute_reply.started": "2025-09-17T14:16:26.904361Z"
    }
   },
   "source": [
    "---\n",
    "title: \"What are tied Embeddings\"\n",
    "date: 2025-09-17\n",
    "categories: [nlp, embeddings, pytorch]\n",
    "description: \"Minimal explanation of tied embeddings in language models with a compact real-model demo.\"\n",
    "page-layout: article\n",
    "title-block-banner: false\n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "  cache: true\n",
    "  freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfebd060-af37-4c77-b8f4-bcaa44b43b9e",
   "metadata": {},
   "source": [
    "Tied embeddings (or *weight tying*) is a small but powerful trick in language models. The idea is to use the same matrix for both the input token embedding and the output softmax layer. This reduces parameters and often improves perplexity.\n",
    "\n",
    "The trick was introduced in 2017 ([Press & Wolf](https://aclanthology.org/E17-2025); [Inan et al.](https://arxiv.org/abs/1611.01462)), and it was already applied in the original *Attention Is All You Need* Transformer.\n",
    "\n",
    "Many popular models such as **GPT-2** and **BERT** also use tied embeddings. However, not all open-source LLMs adopt this practice. For example, **LLaMA** and **Mistral** ship with `tie_word_embeddings = False` in their reference implementations.\n",
    "\n",
    "## A real example\n",
    "\n",
    "To see tied embeddings in action, let’s look at **TinyLlama 1.1B**.  \n",
    "It is small enough to load on a laptop, but large enough that parameter savings are obvious.\n",
    "\n",
    "First, we load the model as-is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81de7d8b-4075-478a-8097-b69900d79eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:26:31.700610Z",
     "iopub.status.busy": "2025-09-17T15:26:31.700359Z",
     "iopub.status.idle": "2025-09-17T15:26:36.528173Z",
     "shell.execute_reply": "2025-09-17T15:26:36.526802Z",
     "shell.execute_reply.started": "2025-09-17T15:26:31.700584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline params: 1100048384\n",
      "Unique params  : 1100048384\n",
      "Embeddings tied? False\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def count_unique_params(model):\n",
    "    seen = set()\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        ptr = p.data_ptr()\n",
    "        if ptr not in seen:\n",
    "            seen.add(ptr)\n",
    "            total += p.numel()\n",
    "    return total\n",
    "\n",
    "def embeddings_share_storage(model):\n",
    "    out = model.get_output_embeddings()\n",
    "    inp = model.get_input_embeddings()\n",
    "    return out is not None and inp is not None and out.weight.data_ptr() == inp.weight.data_ptr()\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "baseline = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print(\"Baseline params:\", count_params(baseline))\n",
    "print(\"Unique params  :\", count_unique_params(baseline))\n",
    "print(\"Embeddings tied?\", embeddings_share_storage(baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad4824-38c0-4079-a5d6-8411055a456f",
   "metadata": {},
   "source": [
    "By default the model loads with separate weights for the input embeddings and the output head. Conceptually the large vocabulary matrix exists twice. Frameworks can report similar totals when modules share a parameter, so we also report a unique parameter count to make the saving explicit. \n",
    "\n",
    "Let’s now tie the embeddings and check the parameter counts again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c916db-fbdd-46fa-842d-4ede6c475111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:26:36.530660Z",
     "iopub.status.busy": "2025-09-17T15:26:36.529630Z",
     "iopub.status.idle": "2025-09-17T15:26:37.616451Z",
     "shell.execute_reply": "2025-09-17T15:26:37.615633Z",
     "shell.execute_reply.started": "2025-09-17T15:26:36.530568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tied params   : 1034512384\n",
      "Unique params : 1034512384\n",
      "Embeddings tied? True\n"
     ]
    }
   ],
   "source": [
    "tied = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# tie input and output embeddings\n",
    "tied.get_output_embeddings().weight = tied.get_input_embeddings().weight\n",
    "if hasattr(tied, \"tie_weights\"):\n",
    "    tied.tie_weights()\n",
    "\n",
    "print(\"Tied params   :\", count_params(tied))\n",
    "print(\"Unique params :\", count_unique_params(tied))\n",
    "print(\"Embeddings tied?\", embeddings_share_storage(tied))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2546b-534d-4e57-94fe-b789d1e8f4fe",
   "metadata": {},
   "source": [
    "Now the input and output embeddings share the same storage.  \n",
    "The unique parameter count drops by about `V × D`, where `V` is the vocabulary size and `D` the embedding dimension.  \n",
    "In TinyLlama this means a saving of over 300 million parameters — the size of the embedding matrix — without changing the model’s behaviour. Let's double check whether this assumption holds water:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cc8ffd-25ae-4510-914a-fa360a81597e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:26:37.617497Z",
     "iopub.status.busy": "2025-09-17T15:26:37.617276Z",
     "iopub.status.idle": "2025-09-17T15:26:37.624283Z",
     "shell.execute_reply": "2025-09-17T15:26:37.623490Z",
     "shell.execute_reply.started": "2025-09-17T15:26:37.617480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size V=32000, embedding dim D=2048\n",
      "Expected saving  : 65,536,000\n",
      "Observed saving  : 65,536,000\n"
     ]
    }
   ],
   "source": [
    "V, D = baseline.get_input_embeddings().weight.shape\n",
    "expected = V * D\n",
    "observed = count_unique_params(baseline) - count_unique_params(tied)\n",
    "print(f\"Vocabulary size V={V}, embedding dim D={D}\")\n",
    "print(f\"Expected saving  : {expected:,}\")\n",
    "print(f\"Observed saving  : {observed:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288bf1e0-bfab-4ba8-9d97-e8e5d760504d",
   "metadata": {},
   "source": [
    "This simple one line change makes the model leaner, saving memory and compute. The table below summarises the effect on parameter size:\n",
    "\n",
    "| Model    | Total params   | Unique params | Embeddings tied |\n",
    "|----------|---------------:|--------------:|:---------------:|\n",
    "| Baseline | 1,100,048,384  | 1,100,048,384 | No              |\n",
    "| Tied     | 1,034,512,384  | 1,034,512,384 | Yes             |\n",
    "\n",
    "Saving: **65,536,000** parameters (= V × D = 32,000 × 2,048).\n",
    "\n",
    "That is why tied embeddings are a default in many architectures, even if some modern open source models leave them disabled. In practice, teams may keep embeddings untied to allocate capacity differently and retain architectural flexibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
